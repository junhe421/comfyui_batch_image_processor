"""
Batch Image Difference to Prompt Files - ComfyUI é«˜çº§å›¾åƒå·®å¼‚åˆ†æèŠ‚ç‚¹
åŸºäº Kontext Bench é£æ ¼çš„æŒ‡ä»¤å¼ prompt ç”Ÿæˆ

å‚è€ƒ: https://huggingface.co/datasets/black-forest-labs/kontext-bench

ä½œè€…ä¿¡æ¯:
ä½œè€…: Asir
QQäº¤æµç¾¤: 960598442
Discord: asir_50811
å…¬ä¼—å·: AsirAI
å…¬ä¼—å·: https://mp.weixin.qq.com/s/iRb8y5LKW46pXbYxTN0iRA

ç‰ˆæœ¬: 2.0.0 - å…¨æ–°æŒ‡ä»¤å¼åˆ†æå¼•æ“
åˆ†ç±»: IO/Batch Processing
"""

import os
import torch
import numpy as np
from PIL import Image, ImageOps
import cv2
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
from collections import defaultdict

# ç§‘å­¦è®¡ç®—åº“å¯¼å…¥
try:
    from skimage.metrics import structural_similarity as ssim
    from skimage import measure, morphology, segmentation
    from skimage.feature import local_binary_pattern
    from scipy import ndimage
except ImportError as e:
    print(f"âš ï¸ scikit-imageåº“æœªå®‰è£…: {e}")
    print("è¯·è¿è¡Œ: pip install scikit-image scipy")
    raise e

# èŠ‚ç‚¹åˆ†ç±»å¸¸é‡
CATEGORY_NAME = "IO/Batch Processing"
SUPPORTED_EXTENSIONS = ['.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp']

# Kontext Bench é£æ ¼çš„æŒ‡ä»¤æ¨¡æ¿
INSTRUCTION_TEMPLATES = {
    'removal': [
        "remove the {object}",
        "delete the {object}",
        "take away the {object}",
        "eliminate the {object} from the image"
    ],
    'addition': [
        "add a {object}",
        "give the {subject} a {object}",
        "put a {object} on the {subject}",
        "place a {object} in the image"
    ],
    'modification': [
        "make the {object} {attribute}",
        "change the {object} to {attribute}",
        "turn the {object} {attribute}",
        "modify the {object} to be {attribute}"
    ],
    'style_transformation': [
        "convert to {style} style",
        "apply {style} effect",
        "transform into {style}",
        "change the style to {style}"
    ],
    'global_change': [
        "adjust the lighting",
        "change the overall appearance",
        "modify the image composition",
        "enhance the visual style"
    ]
}

# å¸¸è§ç‰©ä½“å’Œå±æ€§è¯æ±‡
COMMON_OBJECTS = [
    "cat", "dog", "person", "car", "tree", "building", "flower", "bird", 
    "hat", "shirt", "dress", "chair", "table", "book", "phone", "glass"
]

COMMON_ATTRIBUTES = [
    "bigger", "smaller", "fatter", "thinner", "red", "blue", "green", "yellow",
    "bright", "dark", "colorful", "transparent", "metallic", "wooden"
]

STYLE_TYPES = [
    "cartoon", "anime", "watercolor", "oil painting", "sketch", "comic",
    "vintage", "modern", "artistic", "realistic", "abstract"
]

class BatchImageDifferenceToPromptFiles:
    """
    é«˜çº§æ‰¹é‡å›¾åƒå·®å¼‚åˆ†æèŠ‚ç‚¹ - Kontext Bench é£æ ¼
    
    ä½¿ç”¨å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰ç®—æ³•ç²¾ç¡®è¯†åˆ«å›¾åƒå˜åŒ–ç±»å‹ï¼Œ
    ç”Ÿæˆå‡†ç¡®çš„æŒ‡ä»¤å¼promptï¼Œæ”¯æŒç‰©ä½“çº§åˆ«å’Œé£æ ¼çº§åˆ«çš„å˜åŒ–æ£€æµ‹ã€‚
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "source_folder": ("STRING", {
                    "default": "", 
                    "widget": "directory",
                    "tooltip": "åŸå›¾æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„"
                }),
                "target_folder": ("STRING", {
                    "default": "", 
                    "widget": "directory",
                    "tooltip": "ç›®æ ‡å›¾æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„"
                }),
                "output_folder": ("STRING", {
                    "default": "output/kontext_prompts", 
                    "widget": "directory",
                    "tooltip": "ç”Ÿæˆçš„.txtæŒ‡ä»¤æ–‡ä»¶ä¿å­˜è·¯å¾„"
                }),
                "source_suffix": ("STRING", {
                    "default": "_R", 
                    "tooltip": "è¯†åˆ«åŸå›¾æ–‡ä»¶çš„åç¼€æ ‡è¯†"
                }),
                "target_suffix": ("STRING", {
                    "default": "_T", 
                    "tooltip": "è¯†åˆ«ç›®æ ‡å›¾æ–‡ä»¶çš„åç¼€æ ‡è¯†"
                }),
            },
            "optional": {
                "ssim_threshold": ("FLOAT", {
                    "default": 0.8, 
                    "min": 0.0, 
                    "max": 1.0, 
                    "step": 0.01,
                    "tooltip": "SSIMç›¸ä¼¼æ€§é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¤ä¸ºæœ‰æ˜¾è‘—å˜åŒ–"
                }),
                "contour_min_area": ("INT", {
                    "default": 200,
                    "min": 50,
                    "max": 5000,
                    "step": 50,
                    "tooltip": "ç‰©ä½“å˜åŒ–æ£€æµ‹çš„æœ€å°è½®å»“é¢ç§¯"
                }),
                "style_sensitivity": ("FLOAT", {
                    "default": 0.3,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "é£æ ¼å˜åŒ–æ£€æµ‹æ•æ„Ÿåº¦"
                }),
                "enable_debug": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºè¯¦ç»†åˆ†æä¿¡æ¯"
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("target_images_batch", "analysis_report")
    FUNCTION = "analyze_and_generate_instructions"
    CATEGORY = CATEGORY_NAME
    
    OUTPUT_NODE = False
    
    def __init__(self):
        """åˆå§‹åŒ–é«˜çº§åˆ†æå¼•æ“"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.debug_mode = False
        
        # åˆå§‹åŒ–åˆ†æç»Ÿè®¡
        self.analysis_stats = {
            'removal': 0,
            'addition': 0, 
            'modification': 0,
            'style_transformation': 0,
            'global_change': 0
        }

    def analyze_and_generate_instructions(self, source_folder: str, target_folder: str,
                                        output_folder: str, source_suffix: str, target_suffix: str,
                                        ssim_threshold: float = 0.8, contour_min_area: int = 200,
                                        style_sensitivity: float = 0.3, enable_debug: bool = False) -> Tuple[torch.Tensor, str]:
        """
        æ ¸å¿ƒåˆ†æå‡½æ•° - ç”ŸæˆKontext Benché£æ ¼çš„æŒ‡ä»¤å¼prompt
        
        Args:
            source_folder: åŸå›¾æ–‡ä»¶å¤¹è·¯å¾„
            target_folder: ç›®æ ‡å›¾æ–‡ä»¶å¤¹è·¯å¾„
            output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
            source_suffix: åŸå›¾åç¼€æ ‡è¯†
            target_suffix: ç›®æ ‡å›¾åç¼€æ ‡è¯†
            ssim_threshold: SSIMç›¸ä¼¼æ€§é˜ˆå€¼
            contour_min_area: æœ€å°è½®å»“é¢ç§¯
            style_sensitivity: é£æ ¼å˜åŒ–æ•æ„Ÿåº¦
            enable_debug: è°ƒè¯•æ¨¡å¼
            
        Returns:
            tuple: (ç›®æ ‡å›¾åƒæ‰¹å¤„ç†å¼ é‡, åˆ†ææŠ¥å‘Šå­—ç¬¦ä¸²)
        """
        try:
            self.debug_mode = enable_debug
            self._reset_stats()
            
            # 1. è·¯å¾„éªŒè¯
            self._log_debug("ğŸ” å¼€å§‹é«˜çº§å›¾åƒå·®å¼‚åˆ†æ...")
            validated_paths = self._validate_paths(source_folder, target_folder, output_folder)
            source_folder, target_folder, output_folder = validated_paths
            
            # 2. å›¾åƒå¯¹åŒ¹é…
            matched_pairs = self._match_image_pairs(source_folder, target_folder, source_suffix, target_suffix)
            if not matched_pairs:
                return self._return_error("æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„å›¾åƒå¯¹")
            
            self._log_debug(f"ğŸ“Š æ‰¾åˆ° {len(matched_pairs)} å¯¹å¾…åˆ†æå›¾åƒ")
            
            # 3. é«˜çº§æ‰¹é‡åˆ†æ
            processed_targets = []
            success_count = 0
            error_count = 0
            analysis_details = []
            
            print(f"ğŸ§  å¼€å§‹ Kontext Bench é£æ ¼åˆ†æ {len(matched_pairs)} å¯¹å›¾åƒ...")
            
            for i, (source_path, target_path) in enumerate(matched_pairs):
                try:
                    self._log_debug(f"ğŸ”¬ åˆ†æç¬¬ {i+1}/{len(matched_pairs)} å¯¹: {os.path.basename(target_path)}")
                    
                    # a. åŠ è½½å’Œé¢„å¤„ç†å›¾åƒå¯¹
                    source_img, target_img = self._load_and_preprocess_images(source_path, target_path)
                    
                    # b. æ‰§è¡Œå¤šå±‚æ¬¡å·®å¼‚åˆ†æ
                    analysis_result = self._perform_advanced_difference_analysis(
                        source_img, target_img, ssim_threshold, contour_min_area, style_sensitivity)
                    
                    if analysis_result is None:
                        error_count += 1
                        continue
                    
                    instruction_text, change_type, confidence = analysis_result
                    
                    # c. ä¿å­˜æŒ‡ä»¤æ–‡ä»¶
                    instruction_file_path = self._save_instruction_file(
                        target_path, output_folder, instruction_text)
                    
                    # d. æ”¶é›†ç›®æ ‡å›¾åƒå¼ é‡
                    target_tensor = self._pil_to_tensor(target_img)
                    processed_targets.append(target_tensor)
                    
                    # e. è®°å½•åˆ†æè¯¦æƒ…
                    analysis_details.append({
                        'target_file': os.path.basename(target_path),
                        'instruction_file': os.path.basename(instruction_file_path),
                        'instruction': instruction_text,
                        'change_type': change_type,
                        'confidence': confidence
                    })
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.analysis_stats[change_type] += 1
                    success_count += 1
                    
                except Exception as e:
                    print(f"âš ï¸ åˆ†æå¤±è´¥ {os.path.basename(target_path)}: {str(e)}")
                    error_count += 1
                    continue
            
            # 4. ç”Ÿæˆåˆ†ææŠ¥å‘Š
            if processed_targets:
                target_images_batch = self._create_batch_tensor(processed_targets)
                analysis_report = self._generate_analysis_report(
                    success_count, error_count, output_folder, analysis_details)
                
                print(f"âœ… Kontexté£æ ¼åˆ†æå®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")
                return (target_images_batch, analysis_report)
            else:
                return self._return_error("æ‰€æœ‰å›¾åƒå¯¹åˆ†æå¤±è´¥")
                
        except Exception as e:
            error_msg = f"é«˜çº§åˆ†æå¼•æ“å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return self._return_error(error_msg)

    def _perform_advanced_difference_analysis(self, source_img: Image.Image, target_img: Image.Image,
                                            ssim_threshold: float, contour_min_area: int, 
                                            style_sensitivity: float) -> Optional[Tuple[str, str, float]]:
        """
        æ‰§è¡Œé«˜çº§å·®å¼‚åˆ†æ - æ ¸å¿ƒç®—æ³•å¼•æ“
        
        Args:
            source_img: åŸå›¾PILå¯¹è±¡
            target_img: ç›®æ ‡å›¾PILå¯¹è±¡
            ssim_threshold: SSIMé˜ˆå€¼
            contour_min_area: æœ€å°è½®å»“é¢ç§¯
            style_sensitivity: é£æ ¼æ•æ„Ÿåº¦
            
        Returns:
            tuple: (æŒ‡ä»¤æ–‡æœ¬, å˜åŒ–ç±»å‹, ç½®ä¿¡åº¦) æˆ– None
        """
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            source_array = np.array(source_img)
            target_array = np.array(target_img)
            
            # 1. è®¡ç®—SSIMå·®å¼‚å›¾
            source_gray = cv2.cvtColor(source_array, cv2.COLOR_RGB2GRAY)
            target_gray = cv2.cvtColor(target_array, cv2.COLOR_RGB2GRAY)
            
            ssim_score, diff_map = ssim(source_gray, target_gray, full=True)
            self._log_debug(f"ğŸ“ˆ SSIMç›¸ä¼¼æ€§åˆ†æ•°: {ssim_score:.3f}")
            
            # 2. è®¡ç®—åƒç´ çº§ç»å¯¹å·®å€¼
            pixel_diff = np.abs(source_array.astype(np.float32) - target_array.astype(np.float32))
            pixel_diff_gray = np.mean(pixel_diff, axis=2)
            
            # 3. äºŒå€¼åŒ–å¤„ç†çªå‡ºå˜åŒ–åŒºåŸŸ
            diff_binary = self._create_change_mask(diff_map, pixel_diff_gray, ssim_threshold)
            
            # 4. è½®å»“æ£€æµ‹å’Œåˆ†æ
            contours = self._detect_change_contours(diff_binary, contour_min_area)
            
            # 5. åˆ†æå˜åŒ–ç±»å‹
            change_analysis = self._analyze_change_type(
                source_array, target_array, contours, ssim_score, style_sensitivity)
            
            # 6. ç”ŸæˆæŒ‡ä»¤å¼æè¿°
            instruction = self._generate_instruction_prompt(change_analysis)
            
            return instruction, change_analysis['type'], change_analysis['confidence']
            
        except Exception as e:
            print(f"âš ï¸ é«˜çº§å·®å¼‚åˆ†æå¤±è´¥: {str(e)}")
            return None

    def _create_change_mask(self, ssim_diff: np.ndarray, pixel_diff: np.ndarray, threshold: float) -> np.ndarray:
        """
        åˆ›å»ºå˜åŒ–æ©ç  - ç»“åˆSSIMå’Œåƒç´ å·®å¼‚
        
        Args:
            ssim_diff: SSIMå·®å¼‚å›¾
            pixel_diff: åƒç´ å·®å¼‚å›¾
            threshold: é˜ˆå€¼
            
        Returns:
            np.ndarray: äºŒå€¼åŒ–å˜åŒ–æ©ç 
        """
        # å½’ä¸€åŒ–å·®å¼‚å›¾
        ssim_diff_norm = (1 - ssim_diff) * 255
        pixel_diff_norm = (pixel_diff / pixel_diff.max()) * 255
        
        # ç»„åˆä¸¤ç§å·®å¼‚
        combined_diff = (ssim_diff_norm * 0.6 + pixel_diff_norm * 0.4)
        
        # äºŒå€¼åŒ–
        binary_threshold = int((1 - threshold) * 255)
        _, binary_mask = cv2.threshold(
            combined_diff.astype(np.uint8), binary_threshold, 255, cv2.THRESH_BINARY)
        
        # å½¢æ€å­¦æ“ä½œå»å™ª
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)
        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)
        
        return binary_mask

    def _detect_change_contours(self, binary_mask: np.ndarray, min_area: int) -> List:
        """
        æ£€æµ‹å˜åŒ–è½®å»“
        
        Args:
            binary_mask: äºŒå€¼åŒ–æ©ç 
            min_area: æœ€å°é¢ç§¯
            
        Returns:
            List: æœ‰æ•ˆè½®å»“åˆ—è¡¨
        """
        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # è¿‡æ»¤å°è½®å»“å¹¶æŒ‰é¢ç§¯æ’åº
        valid_contours = [c for c in contours if cv2.contourArea(c) >= min_area]
        valid_contours.sort(key=cv2.contourArea, reverse=True)
        
        self._log_debug(f"ğŸ” æ£€æµ‹åˆ° {len(valid_contours)} ä¸ªæœ‰æ•ˆå˜åŒ–åŒºåŸŸ")
        
        return valid_contours

    def _analyze_change_type(self, source_array: np.ndarray, target_array: np.ndarray,
                           contours: List, ssim_score: float, style_sensitivity: float) -> Dict[str, Any]:
        """
        åˆ†æå˜åŒ–ç±»å‹ - æ ¸å¿ƒåˆ†ç±»é€»è¾‘
        
        Args:
            source_array: åŸå›¾æ•°ç»„
            target_array: ç›®æ ‡å›¾æ•°ç»„
            contours: è½®å»“åˆ—è¡¨
            ssim_score: SSIMåˆ†æ•°
            style_sensitivity: é£æ ¼æ•æ„Ÿåº¦
            
        Returns:
            Dict: å˜åŒ–åˆ†æç»“æœ
        """
        height, width = source_array.shape[:2]
        total_area = height * width
        
        if not contours:
            # æ— æ˜¾è‘—è½®å»“å˜åŒ–ï¼Œå¯èƒ½æ˜¯å…¨å±€é£æ ¼å˜åŒ–
            if ssim_score < 0.9:
                return {
                    'type': 'style_transformation',
                    'confidence': 0.8,
                    'details': 'global_style_change',
                    'major_contour': None
                }
            else:
                return {
                    'type': 'global_change',
                    'confidence': 0.6,
                    'details': 'minimal_change',
                    'major_contour': None
                }
        
        # åˆ†æä¸»è¦è½®å»“
        major_contour = contours[0]
        contour_area = cv2.contourArea(major_contour)
        area_ratio = contour_area / total_area
        
        self._log_debug(f"ğŸ“Š ä¸»è½®å»“é¢ç§¯æ¯”ä¾‹: {area_ratio:.3f}")
        
        # è·å–è½®å»“è¾¹ç•Œæ¡†
        x, y, w, h = cv2.boundingRect(major_contour)
        
        # æå–è½®å»“åŒºåŸŸçš„åƒç´ 
        mask = np.zeros((height, width), dtype=np.uint8)
        cv2.fillPoly(mask, [major_contour], 255)
        
        source_region = source_array[mask > 0]
        target_region = target_array[mask > 0]
        
        # è®¡ç®—åŒºåŸŸå†…çš„é¢œè‰²å˜åŒ–
        source_mean = np.mean(source_region, axis=0)
        target_mean = np.mean(target_region, axis=0)
        color_change = np.linalg.norm(source_mean - target_mean)
        
        # åˆ¤æ–­å˜åŒ–ç±»å‹
        if area_ratio > 0.15:  # å¤§åŒºåŸŸå˜åŒ–
            if color_change < 30:  # é¢œè‰²å˜åŒ–ä¸å¤§
                # å¯èƒ½æ˜¯ç‰©ä½“æ·»åŠ æˆ–ç§»é™¤
                source_brightness = np.mean(source_region)
                target_brightness = np.mean(target_region)
                
                if target_brightness < source_brightness - 20:
                    change_type = 'removal'
                    confidence = 0.9
                elif target_brightness > source_brightness + 20:
                    change_type = 'addition'
                    confidence = 0.9
                else:
                    change_type = 'modification'
                    confidence = 0.8
            else:  # æ˜¾è‘—é¢œè‰²å˜åŒ–
                change_type = 'modification'
                confidence = 0.9
        else:  # å°åŒºåŸŸå˜åŒ–æˆ–åˆ†å¸ƒå¼å˜åŒ–
            if len(contours) > 10:  # å¤šä¸ªå°å˜åŒ–åŒºåŸŸ
                change_type = 'style_transformation'
                confidence = 0.8
            else:
                change_type = 'modification'
                confidence = 0.7
        
        return {
            'type': change_type,
            'confidence': confidence,
            'details': {
                'area_ratio': area_ratio,
                'color_change': color_change,
                'contour_count': len(contours),
                'bbox': (x, y, w, h)
            },
            'major_contour': major_contour
        }

    def _generate_instruction_prompt(self, change_analysis: Dict[str, Any]) -> str:
        """
        ç”ŸæˆKontext Benché£æ ¼çš„æŒ‡ä»¤å¼prompt
        
        Args:
            change_analysis: å˜åŒ–åˆ†æç»“æœ
            
        Returns:
            str: æŒ‡ä»¤å¼promptæ–‡æœ¬
        """
        change_type = change_analysis['type']
        confidence = change_analysis['confidence']
        
        # æ ¹æ®å˜åŒ–ç±»å‹ç”ŸæˆæŒ‡ä»¤
        if change_type == 'removal':
            return self._generate_removal_instruction(change_analysis)
        elif change_type == 'addition':
            return self._generate_addition_instruction(change_analysis)
        elif change_type == 'modification':
            return self._generate_modification_instruction(change_analysis)
        elif change_type == 'style_transformation':
            return self._generate_style_instruction(change_analysis)
        else:  # global_change
            return self._generate_global_instruction(change_analysis)

    def _generate_removal_instruction(self, analysis: Dict) -> str:
        """ç”Ÿæˆç§»é™¤ç±»æŒ‡ä»¤"""
        templates = INSTRUCTION_TEMPLATES['removal']
        
        # æ ¹æ®åŒºåŸŸå¤§å°æ¨æµ‹ç‰©ä½“ç±»å‹
        details = analysis.get('details', {})
        area_ratio = details.get('area_ratio', 0)
        
        if area_ratio > 0.3:
            obj = "main object"
        elif area_ratio > 0.1:
            obj = np.random.choice(COMMON_OBJECTS)
        else:
            obj = "small object"
        
        template = np.random.choice(templates)
        return template.format(object=obj)

    def _generate_addition_instruction(self, analysis: Dict) -> str:
        """ç”Ÿæˆæ·»åŠ ç±»æŒ‡ä»¤"""
        templates = INSTRUCTION_TEMPLATES['addition']
        
        details = analysis.get('details', {})
        area_ratio = details.get('area_ratio', 0)
        
        if area_ratio > 0.2:
            obj = np.random.choice(["hat", "shirt", "accessory"])
            subject = "person"
        else:
            obj = np.random.choice(["small item", "decoration", "detail"])
            subject = "image"
        
        template = np.random.choice(templates)
        return template.format(object=obj, subject=subject)

    def _generate_modification_instruction(self, analysis: Dict) -> str:
        """ç”Ÿæˆä¿®æ”¹ç±»æŒ‡ä»¤"""
        templates = INSTRUCTION_TEMPLATES['modification']
        
        details = analysis.get('details', {})
        color_change = details.get('color_change', 0)
        
        if color_change > 50:
            obj = np.random.choice(COMMON_OBJECTS)
            attr = np.random.choice(["red", "blue", "green", "colorful", "bright"])
        else:
            obj = "object"
            attr = np.random.choice(COMMON_ATTRIBUTES)
        
        template = np.random.choice(templates)
        return template.format(object=obj, attribute=attr)

    def _generate_style_instruction(self, analysis: Dict) -> str:
        """ç”Ÿæˆé£æ ¼è½¬æ¢æŒ‡ä»¤"""
        templates = INSTRUCTION_TEMPLATES['style_transformation']
        
        style = np.random.choice(STYLE_TYPES)
        template = np.random.choice(templates)
        return template.format(style=style)

    def _generate_global_instruction(self, analysis: Dict) -> str:
        """ç”Ÿæˆå…¨å±€å˜åŒ–æŒ‡ä»¤"""
        templates = INSTRUCTION_TEMPLATES['global_change']
        return np.random.choice(templates)

    def _save_instruction_file(self, target_path: str, output_folder: str, instruction: str) -> str:
        """
        ä¿å­˜æŒ‡ä»¤æ–‡ä»¶
        
        Args:
            target_path: ç›®æ ‡å›¾è·¯å¾„
            output_folder: è¾“å‡ºæ–‡ä»¶å¤¹
            instruction: æŒ‡ä»¤æ–‡æœ¬
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            target_filename = os.path.basename(target_path)
            instruction_filename = os.path.splitext(target_filename)[0] + '.txt'
            instruction_file_path = os.path.join(output_folder, instruction_filename)
            
            with open(instruction_file_path, 'w', encoding='utf-8') as f:
                f.write(instruction)
                
            self._log_debug(f"ğŸ’¾ æŒ‡ä»¤å·²ä¿å­˜: {instruction_filename}")
            return instruction_file_path
            
        except Exception as e:
            raise IOError(f"ä¿å­˜æŒ‡ä»¤æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _reset_stats(self):
        """é‡ç½®åˆ†æç»Ÿè®¡"""
        for key in self.analysis_stats:
            self.analysis_stats[key] = 0

    def _validate_paths(self, source_folder: str, target_folder: str, output_folder: str) -> Tuple[str, str, str]:
        """è·¯å¾„éªŒè¯"""
        if not source_folder or not os.path.exists(source_folder):
            raise FileNotFoundError(f"åŸå›¾æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {source_folder}")
        if not target_folder or not os.path.exists(target_folder):
            raise FileNotFoundError(f"ç›®æ ‡å›¾æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {target_folder}")
            
        if not output_folder:
            output_folder = "output/kontext_prompts"
        os.makedirs(output_folder, exist_ok=True)
        
        return source_folder, target_folder, output_folder

    def _match_image_pairs(self, source_folder: str, target_folder: str, 
                          source_suffix: str, target_suffix: str) -> List[Tuple[str, str]]:
        """åŒ¹é…å›¾åƒå¯¹"""
        matched_pairs = []
        
        target_files = [f for f in os.listdir(target_folder) 
                       if any(f.lower().endswith(ext) for ext in SUPPORTED_EXTENSIONS)]
        
        for target_file in target_files:
            if target_suffix not in target_file:
                continue
                
            file_base = target_file.rsplit('.', 1)[0]
            file_ext = '.' + target_file.rsplit('.', 1)[1]
            
            if not file_base.endswith(target_suffix):
                continue
                
            prefix = file_base[:-len(target_suffix)]
            source_file = prefix + source_suffix + file_ext
            source_path = os.path.join(source_folder, source_file)
            target_path = os.path.join(target_folder, target_file)
            
            if os.path.exists(source_path):
                matched_pairs.append((source_path, target_path))
                self._log_debug(f"âœ… åŒ¹é…: {source_file} <-> {target_file}")
                
        return matched_pairs

    def _load_and_preprocess_images(self, source_path: str, target_path: str) -> Tuple[Image.Image, Image.Image]:
        """åŠ è½½å’Œé¢„å¤„ç†å›¾åƒå¯¹"""
        source_img = Image.open(source_path).convert('RGB')
        target_img = Image.open(target_path).convert('RGB')
        
        if source_img.size != target_img.size:
            source_img = source_img.resize(target_img.size, Image.LANCZOS)
            
        return source_img, target_img

    def _pil_to_tensor(self, pil_image: Image.Image) -> torch.Tensor:
        """PILè½¬å¼ é‡"""
        image_array = np.array(pil_image).astype(np.float32) / 255.0
        return torch.from_numpy(image_array)

    def _create_batch_tensor(self, image_tensors: List[torch.Tensor]) -> torch.Tensor:
        """åˆ›å»ºæ‰¹å¤„ç†å¼ é‡"""
        if not image_tensors:
            raise ValueError("å›¾åƒå¼ é‡åˆ—è¡¨ä¸ºç©º")
        return torch.stack(image_tensors, dim=0)

    def _generate_analysis_report(self, success_count: int, error_count: int,
                                output_folder: str, analysis_details: List[Dict]) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_lines = [
            "ğŸ§  Kontext Bench é£æ ¼å›¾åƒå·®å¼‚åˆ†ææŠ¥å‘Š",
            "=" * 60,
            f"âœ… æˆåŠŸåˆ†æ: {success_count} ä¸ªå›¾åƒå¯¹",
            f"âŒ åˆ†æå¤±è´¥: {error_count} ä¸ªå›¾åƒå¯¹",
            f"ğŸ“ è¾“å‡ºç›®å½•: {output_folder}",
            "",
            "ğŸ“Š å˜åŒ–ç±»å‹ç»Ÿè®¡:",
            "-" * 30,
            f"ğŸ”´ ç‰©ä½“ç§»é™¤ (Removal): {self.analysis_stats['removal']} ä¸ª",
            f"ğŸŸ¢ ç‰©ä½“æ·»åŠ  (Addition): {self.analysis_stats['addition']} ä¸ª",
            f"ğŸŸ¡ ç‰©ä½“ä¿®æ”¹ (Modification): {self.analysis_stats['modification']} ä¸ª",
            f"ğŸ¨ é£æ ¼è½¬æ¢ (Style): {self.analysis_stats['style_transformation']} ä¸ª",
            f"ğŸŒ å…¨å±€å˜åŒ– (Global): {self.analysis_stats['global_change']} ä¸ª",
            ""
        ]
        
        if analysis_details:
            report_lines.extend([
                "ğŸ“ ç”Ÿæˆçš„æŒ‡ä»¤ç¤ºä¾‹:",
                "-" * 30
            ])
            
            for i, detail in enumerate(analysis_details[:8], 1):
                report_lines.append(f"{i:2d}. {detail['target_file']}")
                report_lines.append(f"    ğŸ“‹ {detail['instruction']}")
                report_lines.append(f"    ğŸ”– ç±»å‹: {detail['change_type']} (ç½®ä¿¡åº¦: {detail['confidence']:.2f})")
                report_lines.append("")
                
            if len(analysis_details) > 8:
                report_lines.append(f"... ä»¥åŠå…¶ä»– {len(analysis_details) - 8} ä¸ªæŒ‡ä»¤æ–‡ä»¶")
                
        report_lines.extend([
            "",
            "ğŸ¯ Kontext Bench é£æ ¼åˆ†æå®Œæˆï¼",
            "ğŸ“– å‚è€ƒ: https://huggingface.co/datasets/black-forest-labs/kontext-bench"
        ])
        
        return "\n".join(report_lines)

    def _return_error(self, error_message: str) -> Tuple[torch.Tensor, str]:
        """é”™è¯¯è¿”å›"""
        empty_image = torch.zeros((1, 64, 64, 3), dtype=torch.float32)
        error_report = f"âŒ åˆ†æå¤±è´¥: {error_message}\n\nè¯·æ£€æŸ¥è¾“å…¥å‚æ•°å¹¶é‡è¯•ã€‚"
        return (empty_image, error_report)

    def _log_debug(self, message: str):
        """è°ƒè¯•æ—¥å¿—"""
        if self.debug_mode:
            print(f"ğŸ” DEBUG: {message}")


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "BatchImageDifferenceToPromptFiles": BatchImageDifferenceToPromptFiles
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "BatchImageDifferenceToPromptFiles": "Batch Diff Prompt to Files (Kontext Style)"
}

__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS"] 